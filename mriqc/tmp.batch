#!/bin/bash

#SBATCH -J mriqc_participant
#SBATCH -p russpold,owners,normal
#SBATCH --time=48:00:00
#SBATCH -n 1
#SBATCH --cpus-per-task=10
#SBATCH --ntasks-per-node=1
#SBATCH --mem-per-cpu=6400M

# Outputs ----------------------------------
#SBATCH -o ../.out/mriqc-tmp.out
#SBATCH -e ../.err/mriqc-tmp.err
#SBATCH --mail-user=zenkavi@stanford.edu
#SBATCH --mail-type=FAIL
# ------------------------------------------

module load system

unset PYTHONPATH
export DATA_LOC=/oak/stanford/groups/russpold/data/ds000054/0.0.1/

rsync -r $DATA_LOC/dataset_description.json $HOME/data/
rsync -r $DATA_LOC/participants.json $HOME/data/
rsync -r $DATA_LOC/participants.tsv $HOME/data/
rsync -r $DATA_LOC/phasediff.json $HOME/data/
rsync -r $DATA_LOC/task-bart_events.json $HOME/data/
rsync -r $DATA_LOC/task-machinegame_events.json $HOME/data/
rsync -r $DATA_LOC/task-machinegame_bold.json $HOME/data/
rsync -r $DATA_LOC/task-machinegame_bold.json $HOME/data/
rsync -r $DATA_LOC/sub-100207 $HOME/data/

singularity run /share/PI/russpold/singularity_images/poldracklab_mriqc_0.14.2-2018-08-21-070e53b20a43.img $HOME/data/ $HOME/out/ participant -w $HOME/work --participant_label 100207 --mem_gb 50 --n_procs 10 --ants-nthreads 8 -vvv

rsync -r $HOME/out/ /oak/stanford/groups/russpold/data/ds000054/0.0.1/derivatives/mriqc_0.14.2
